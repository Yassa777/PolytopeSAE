# configs/v2-focused.yaml
# Focused 25-30 GPU-hour experiment based on V2 specification

run:
  seed: 123
  device: cuda:0
  budget_hours: 30
  compile: false  # Disable compilation for short runs (overhead dominates)
  notes: "V2 focused experiment: teacher extraction + baseline vs teacher-init H-SAE comparison"

model:
  name: EleutherAI/pythia-410m  # Pythia 410M parameters (much smaller, faster)
  dtype: bf16                  # SANITY: All tensors must match dtype (torch.zeros(..., dtype=x.dtype))
  activation_hook: final_residual_pre_unembed
  layers_probe: ["last"]  # Single late layer only

data:
  token_budget: 8_000_000  # Reduced for focused experiment
  window_length: 128
  shard_size_tokens: 262_144
  source: "Wikipedia 20231101 + curated prompts"

dataloader:
  num_workers: 6           # Optimized for small model training
  persistent_workers: true # Keep workers alive between epochs
  pin_memory: true         # Faster GPU transfer

concepts:
  ontology: wordnet
  parents: 5                     # Match Phase 1 actual extraction (dry run scale)
  max_children_per_parent: 2     # Match what we actually extracted  
  prompts_per_concept: 64        # Reduced from 128 to 64
  split: {train: 0.70, val: 0.15, test: 0.15}

geometry:
  whitening: unembedding_rows
  shrinkage: 0.05
  estimator:
    type: lda
    lda_shrinkage: 0.10
    min_vocab_count: 50
    class_balance: true
    energy_threshold: 0.85  # SVD energy threshold for projector dimensions

teacher:
  parent_vectors: "from_final_layer"
  child_deltas: "LDA_in_parent_subspace"
  normalize_causal: true

hsae:
  # IDENTICAL architecture for Phase 2 & 3 comparison (following geometric story K=1)
  # Adjusted for Pythia-410M (hidden_size=1024)
  m_top: 16                  # Reduced for smaller model
  topk_parent: 1             # K=1 aligns with geometry & ablations
  subspace_dim: 32           # Reduced for smaller model (33 active dims/token)
  m_low: 8                   # Keep same child choices per parent
  topk_child: 1              # K=1 for geometric consistency
  projectors_init: "svd_from_teacher"
  l1_parent: 2.0e-4          # Reduced, will add warmup
  l1_child: 2.0e-4           # Reduced, will add warmup  
  biorth_lambda: 1.0e-4      # Keep low until EV fits
  causal_ortho_lambda: 2.0e-4  # Applied only during freeze stage
  router_temp: {start: 5.0, end: 0.7}  # Higher start to encourage exploration in Stage A
  top_level_beta: 0.3        # Emphasize reconstruction
  
  # Our default settings (teacher-init friendly)
  use_tied_decoders_parent: false
  use_tied_decoders_child: false
  tie_projectors: false
  use_decoder_bias: true
  use_offdiag_biorth: false

training:
  optimizer: adamw
  lr: 2.5e-4                 # Stable learning rate
  weight_decay: 1.0e-4
  grad_clip: 1.0
  batch_size_acts: 1024      # SANITY: Small enough for val (ensure ≤ len(val_ds))
  grad_accum_steps: 4        # Use grad accumulation instead of large batches (effective=4096)
  lr_schedule: "cosine"      # Add cosine annealing
  lr_warmup_steps: 1000      # Longer warmup for stability
  l1_warmup_steps: 1000      # Warmup L1 from 0 → target over 1k steps (≥ stabilize_steps)
  
  # Fast training settings for small model
  val_every: 500             # Less frequent validation
  ckpt_every: 2000           # Less frequent checkpointing
  
  # Baseline H-SAE training - FAST SCHEDULE
  baseline:
    total_steps: 5000        # Reduced for faster experimentation
    init_method: "random"
    schedule_only_freeze: true  # run baseline with same freeze→adapt schedule
    stabilize_steps: 500     # Stage A: cheap decoder-frozen stabilization
    adapt_steps: 4500        # Stage B: remaining steps for adaptation
  
  # Teacher-initialized H-SAE training  
  teacher_init:
    total_steps: 5000        # Matched with baseline for fair comparison
    teacher_noise_deg: [0, 5, 15, 30]  # ablation: angular noise on teacher
    stage_A:
      freeze_decoder: true
      steps: 500             # Cheap stabilization (was 2000)
      lr_mult: 0.5
      enable_causal_ortho: true
    stage_B:
      freeze_decoder: false
      steps: 4500            # Main adaptation phase
      lr_mult: 1.0
      enable_causal_ortho: false
      decoder_lr_mult: 0.5  # Lower LR for decoder adaptation

eval:
  metrics: ["1-EV", "1-CE", "purity", "leakage", "steering_leakage"]
  steering_layers: ["last"]
  steering_magnitudes: [0.5, 1.0, 2.0]
  validation:
    angle_threshold_deg: 80
    kl_threshold: 0.10
    controls:
      n_shuffles: 100
      
  targets:
    # Acceptance-oriented targets from V2
    median_angle_deg: 80        # >= 80 degrees
    purity_improvement: 0.10    # +10pp with teacher-init
    leakage_reduction: 0.20     # -20% reduction
    steering_leakage_reduction: 0.20  # -20% reduction

ablate:
  euclidean_vs_causal: true    # Critical for demonstrating necessity
  topk_1_vs_2: true
  no_teacher_init: true
  randomize_projectors: true

logging:
  save_dir: "runs/v2_focused"
  log_every: 200              # Validation every 200 steps (was causing slowdown at 50)
  checkpoint_every: 2000      # Less frequent checkpointing
  keep_last_k: 3

  # Specific logging for phases
  phase_1_log: "teacher_extraction"
  phase_2_log: "baseline_hsae" 
  phase_3_log: "teacher_init_hsae"
  phase_4_log: "evaluation_steering"

# Sanity checker configuration - gentler thresholds for Stage A
sanity_checker:
  check_interval: 600         # Delayed checks to allow stabilization
  max_restarts: 1             # Reduced restarts to prevent loops (set to 0 to disable)
  min_route_usage_pct: 15     # Realistic for Top-K=1 early training
  min_active_dims_pct: 20     # Realistic for frozen decoders

# W&B configuration
wandb_project: "polytope-hsae"