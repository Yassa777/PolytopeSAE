# configs/v2-focused.yaml
# Focused 25-30 GPU-hour experiment based on V2 specification

run:
  seed: 123
  device: cuda:0
  budget_hours: 30
  compile: false  # Disable compilation for short runs (overhead dominates)
  notes: "V2 focused experiment: teacher extraction + baseline vs teacher-init H-SAE comparison"

model:
  name: EleutherAI/pythia-410m  # Pythia 410M parameters (much smaller, faster)
  dtype: bf16                  # SANITY: All tensors must match dtype (torch.zeros(..., dtype=x.dtype))
  activation_hook: final_residual_pre_unembed
  layers_probe: ["last"]  # Single late layer only

data:
  token_budget: 16_000_000  # Increased for stronger teacher & eval stability
  window_length: 128
  shard_size_tokens: 262_144
  source: "Wikipedia 20231101 + curated prompts"

dataloader:
  num_workers: 6           # Optimized for small model training
  persistent_workers: true # Keep workers alive between epochs
  pin_memory: true         # Faster GPU transfer

concepts:
  ontology: wordnet
  parents: 16                    # Increase Phase 1 parents to match H-SAE m_top
  max_children_per_parent: 8     # Increase to match H-SAE m_low
  prompts_per_concept: 128       # More samples per concept for better LDA
  split: {train: 0.70, val: 0.15, test: 0.15}

geometry:
  whitening: unembedding_rows
  shrinkage: 0.05
  estimator:
    type: lda
    lda_shrinkage: 0.10
    min_vocab_count: 50
    class_balance: true
    energy_threshold: 0.85  # SVD energy threshold for projector dimensions

teacher:
  parent_vectors: "from_final_layer"
  child_deltas: "LDA_in_parent_subspace"
  normalize_causal: true

hsae:
  # IDENTICAL architecture for Phase 2 & 3 comparison (following geometric story K=1)
  # Adjusted for Pythia-410M (hidden_size=1024)
  m_top: 16                  # Reduced for smaller model
  topk_parent: 1             # K=1 aligns with geometry & ablations
  subspace_dim: 32           # Restore capacity for decoder decorrelation
  m_low: 8                   # Keep same child choices per parent
  topk_child: 1              # K=1 for geometric consistency
  projectors_init: "svd_from_teacher"
  l1_parent: 2.0e-4          # Reduced, will add warmup
  l1_child: 2.0e-4           # Reduced, will add warmup  
  biorth_lambda: 5.0e-4      # Moderate cross-orthogonality
  causal_ortho_lambda: 2.0e-4  # Applied only during freeze stage
  router_temp: {start: 5.0, end: 0.7}  # Higher start to encourage exploration in Stage A
  top_level_beta: 0.1        # Emphasize reconstruction (lower in Stage B)
  
  # Our default settings (teacher-init friendly)
  use_tied_decoders_parent: false
  use_tied_decoders_child: false
  tie_projectors: false
  use_decoder_bias: true
  use_offdiag_biorth: false

training:
  optimizer: adamw
  lr: 3.0e-4                 # Slightly higher LR for faster alignment
  weight_decay: 1.0e-4
  grad_clip: 1.0
  batch_size_acts: 1024      # SANITY: Small enough for val (ensure ≤ len(val_ds))
  grad_accum_steps: 4        # Use grad accumulation instead of large batches (effective=4096)
  lr_schedule: "cosine"      # Add cosine annealing
  lr_warmup_steps: 1000      # Longer warmup for stability
  l1_warmup_steps: 2000      # Longer L1 warmup for baseline stability
  
  # Fast training settings for small model
  val_every: 500             # Less frequent validation
  ckpt_every: 2000           # Less frequent checkpointing
  
  # Baseline H-SAE training - FAST SCHEDULE
  baseline:
    total_steps: 10000       # More steps for better EV and orthogonality
    init_method: "random"
    schedule_only_freeze: true  # run baseline with same freeze→adapt schedule
    stabilize_steps: 500     # Stage A: cheap decoder-frozen stabilization
    adapt_steps: 9500        # Stage B: remaining steps for adaptation
  
  # Teacher-initialized H-SAE training  
  teacher_init:
    total_steps: 10000       # Match baseline for fair comparison
    teacher_noise_deg: [0]             # clean teacher only for main run
    l1_warmup_steps: 4000    # Extended L1 warmup for teacher-init stability
    stage_A:
      freeze_decoder: true
      steps: 500             # Cheap stabilization (was 2000)
      lr_mult: 0.5
      enable_causal_ortho: true
      top_level_beta: 0.30   # stronger parent-only parity early
    stage_B:
      freeze_decoder: false
      steps: 9500            # Main adaptation phase
      lr_mult: 1.0
      enable_causal_ortho: false
      decoder_lr_mult: 1.0  # Allow decoder to adapt fully
      top_level_beta: 0.05   # relax parent-only anchor for EV
    
    # Override sanity checker for teacher-init (disable to prevent restarts)
    sanity_checker:
      enabled: false
      check_interval: 1500     # Very delayed checks for teacher-init stability
      max_restarts: 1
      min_route_usage_pct: 15  # Gentle for teacher-init early stages
      min_active_dims_pct: 20  # Gentle for teacher-init early stages

eval:
  metrics: ["1-EV", "1-CE", "purity", "leakage", "steering_leakage"]
  steering_layers: ["last"]
  steering_magnitudes: [0.5, 1.0, 2.0]
  validation:
    angle_threshold_deg: 80
    kl_threshold: 0.10
    controls:
      n_shuffles: 100
      
  targets:
    # Acceptance-oriented targets from V2
    median_angle_deg: 80        # >= 80 degrees
    purity_improvement: 0.10    # +10pp with teacher-init
    leakage_reduction: 0.20     # -20% reduction
    steering_leakage_reduction: 0.20  # -20% reduction
  assertions:
    identity_max_error_tol: 0.03

ablate:
  euclidean_vs_causal: true    # Critical for demonstrating necessity
  topk_1_vs_2: true
  no_teacher_init: true
  randomize_projectors: true

logging:
  save_dir: "runs/v2_focused"
  log_every: 200              # Validation every 200 steps (was causing slowdown at 50)
  checkpoint_every: 2000      # Less frequent checkpointing
  keep_last_k: 3

  # Specific logging for phases
  phase_1_log: "teacher_extraction"
  phase_2_log: "baseline_hsae" 
  phase_3_log: "teacher_init_hsae_clean"
  phase_4_log: "evaluation_steering"

# Sanity checker configuration
sanity_checker:
  # Phase 2 (baseline): DISABLED - let it run to completion without intervention
  enabled: false              # Completely disable sanity checker for baseline runs
  
  # Phase 3 (teacher-init): ENABLED with relaxed settings  
  # (uncomment and set enabled: true for teacher-init runs)
  # enabled: true
  # check_interval: 1500       # Very delayed checks for teacher-init stability
  # max_restarts: 1           
  # min_route_usage_pct: 15   # Gentle for teacher-init early stages
  # min_active_dims_pct: 20   # Gentle for teacher-init early stages

# W&B configuration
wandb_project: "polytope-hsae"
